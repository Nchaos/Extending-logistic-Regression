{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Logistic Regression: 2009 American Community Survey\n",
    "\n",
    "### Nick Chao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation and Overview (20 points total)\n",
    "\n",
    "[10 points] Explain the task and what business-case or use-case it is designed to solve (or designed to investigate). Detail exactly what the classification task is and what parties would be interested in the results.\n",
    "\n",
    "[5 points] (mostly the same processes as from previous labs) Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis. Describe the final dataset that is used for classification/regression (include a description of any newly formed variables you created).\n",
    "\n",
    "[5 points] Divide you data into training and testing data using an 80% training and 20% testing split. Use the cross validation modules that are part of scikit-learn. Argue \"for\" or \"against\" splitting your data using an 80/20 split. That is, why is the 80/20 split appropriate (or not) for your dataset? \n",
    "\n",
    "\n",
    "### Modeling (50 points total)\n",
    "\n",
    "[20 points] Create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. Use object oriented conventions identical to scikit-learn. You should start with the template developed by the instructor in the course. You should add the following functionality to the logistic regression classifier:\n",
    "Ability to choose optimization technique when class is instantiated: either steepest descent, stochastic gradient descent, or Newton's method. \n",
    "Update the gradient calculation to include a customizable regularization term (either using no regularization, L1 regularization, L2 regularization, or both L1 and L2 regularization). Associate a cost with the regularization term, \"C\", that can be adjusted when the class is instantiated.  \n",
    "\n",
    "[15 points] Train your classifier to achieve good generalization performance. That is, adjust the optimization technique and the value of the regularization term \"C\" to achieve the best performance on your test set. Visualize the performance of the classifier versus the parameters you investigated. Is your method of selecting parameters justified? That is, do you think there is any \"data snooping\" involved with this method of selecting parameters?\n",
    "\n",
    "[15 points] Compare the performance of your \"best\" logistic regression optimization procedure to the procedure used in scikit-learn. Visualize the performance differences in terms of training time and classification performance. Discuss the results. \n",
    "\n",
    "### Deployment (10 points total)\n",
    "Which implementation of logistic regression would you advise be used in a deployed machine learning model, your implementation or scikit-learn (or other third party)? Why?\n",
    "\n",
    "### Exceptional Work (10 points total)\n",
    "You have free reign to provide additional analyses. One idea: Update the code to use either \"one-versus-all\" or \"one-versus-one\" extensions of binary to multi-class classification. \n",
    "One idea (required for 7000 level students): Implement an optimization technique for logistic regression using mean square error as your objective function (instead of binary entropy). Your solution should be able to solve the binary logistic regression problem in one gradient update step. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this lab, we investigate possible relationships between an individual's income and attributes about them. The data used in this lab is provided by the 2009 American Community Survey 1-Year PUMS Population File which can be found here. https://catalog.data.gov/dataset/2009-american-community-survey-1-year-pums-population-file. This dataset contains more than 3 million entries and nearly 300 attributes. To make sense of some of these attributes, there is a dictionary lookup that provides more information about the columns. You can find this reference here. https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMSDataDict09.pdf.\n",
    "\n",
    "To be more specific, we want to see if we can predict a person's current income based on factors about themselves that they might give away when applying for a new job. For example, their age, sex, education level, and more are just a few peices of information that companies may ask for when applying for a new job. \n",
    "\n",
    "This information could be incrediably useful for a company that is hiring new personal. If they know the current income of someone who has applied for a job at their company, then they can offer the lowest starting salary they believe the new canadate will accept. (i.e. slightly above what they currently make). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependancies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.optimize import fmin_bfgs\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.6 s\n",
      "Wall time: 27.8 s\n"
     ]
    }
   ],
   "source": [
    "%time dataA = pd.read_csv('../data/ss09pusa.csv')\n",
    "%time dataB = pd.read_csv('../data/ss09pusb.csv')\n",
    "merged = pd.concat([dataA,dataB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3030728 entries, 0 to 1466654\n",
      "Columns: 279 entries, RT to pwgtp80\n",
      "dtypes: float64(86), int64(190), object(3)\n",
      "memory usage: 6.3+ GB\n"
     ]
    }
   ],
   "source": [
    "merged.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2009 American Community Survey has a lot of data, look at all these columns! \n",
      "\n",
      "['RT', 'SERIALNO', 'SPORDER', 'PUMA', 'ST', 'ADJINC', 'PWGTP', 'AGEP', 'CIT', 'CITWP', 'COW', 'DDRS', 'DEAR', 'DEYE', 'DOUT', 'DPHY', 'DRAT', 'DRATX', 'DREM', 'ENG', 'FER', 'GCL', 'GCM', 'GCR', 'HINS1', 'HINS2', 'HINS3', 'HINS4', 'HINS5', 'HINS6', 'HINS7', 'INTP', 'JWMNP', 'JWRIP', 'JWTR', 'LANX', 'MAR', 'MARHD', 'MARHM', 'MARHT', 'MARHW', 'MARHYP', 'MIG', 'MIL', 'MLPA', 'MLPB', 'MLPC', 'MLPD', 'MLPE', 'MLPF', 'MLPG', 'MLPH', 'MLPI', 'MLPJ', 'MLPK', 'NWAB', 'NWAV', 'NWLA', 'NWLK', 'NWRE', 'OIP', 'PAP', 'REL', 'RETP', 'SCH', 'SCHG', 'SCHL', 'SEMP', 'SEX', 'SSIP', 'SSP', 'WAGP', 'WKHP', 'WKL', 'WKW', 'WRK', 'YOEP', 'ANC', 'ANC1P', 'ANC2P', 'DECADE', 'DIS', 'DRIVESP', 'ESP', 'ESR', 'FOD1P', 'FOD2P', 'HICOV', 'HISP', 'INDP', 'JWAP', 'JWDP', 'LANP', 'MIGPUMA', 'MIGSP', 'MSP', 'NAICSP', 'NATIVITY', 'NOP', 'OC', 'OCCP', 'PAOC', 'PERNP', 'PINCP', 'POBP', 'POVPIP', 'POWPUMA', 'POWSP', 'PRIVCOV', 'PUBCOV', 'QTRBIR', 'RAC1P', 'RAC2P', 'RAC3P', 'RACAIAN', 'RACASN', 'RACBLK', 'RACNHPI', 'RACNUM', 'RACSOR', 'RACWHT', 'RC', 'SCIENGP', 'SCIENGRLP', 'SFN', 'SFR', 'SOCP', 'VPS', 'WAOB', 'FAGEP', 'FANCP', 'FCITP', 'FCITWP', 'FCOWP', 'FDDRSP', 'FDEARP', 'FDEYEP', 'FDOUTP', 'FDPHYP', 'FDRATP', 'FDRATXP', 'FDREMP', 'FENGP', 'FESRP', 'FFERP', 'FFODP', 'FGCLP', 'FGCMP', 'FGCRP', 'FHINS1P', 'FHINS2P', 'FHINS3C', 'FHINS3P', 'FHINS4C', 'FHINS4P', 'FHINS5C', 'FHINS5P', 'FHINS6P', 'FHINS7P', 'FHISP', 'FINDP', 'FINTP', 'FJWDP', 'FJWMNP', 'FJWRIP', 'FJWTRP', 'FLANP', 'FLANXP', 'FMARHDP', 'FMARHMP', 'FMARHTP', 'FMARHWP', 'FMARHYP', 'FMARP', 'FMIGP', 'FMIGSP', 'FMILPP', 'FMILSP', 'FOCCP', 'FOIP', 'FPAP', 'FPOBP', 'FPOWSP', 'FRACP', 'FRELP', 'FRETP', 'FSCHGP', 'FSCHLP', 'FSCHP', 'FSEMP', 'FSEXP', 'FSSIP', 'FSSP', 'FWAGP', 'FWKHP', 'FWKLP', 'FWKWP', 'FWRKP', 'FYOEP', 'pwgtp1', 'pwgtp2', 'pwgtp3', 'pwgtp4', 'pwgtp5', 'pwgtp6', 'pwgtp7', 'pwgtp8', 'pwgtp9', 'pwgtp10', 'pwgtp11', 'pwgtp12', 'pwgtp13', 'pwgtp14', 'pwgtp15', 'pwgtp16', 'pwgtp17', 'pwgtp18', 'pwgtp19', 'pwgtp20', 'pwgtp21', 'pwgtp22', 'pwgtp23', 'pwgtp24', 'pwgtp25', 'pwgtp26', 'pwgtp27', 'pwgtp28', 'pwgtp29', 'pwgtp30', 'pwgtp31', 'pwgtp32', 'pwgtp33', 'pwgtp34', 'pwgtp35', 'pwgtp36', 'pwgtp37', 'pwgtp38', 'pwgtp39', 'pwgtp40', 'pwgtp41', 'pwgtp42', 'pwgtp43', 'pwgtp44', 'pwgtp45', 'pwgtp46', 'pwgtp47', 'pwgtp48', 'pwgtp49', 'pwgtp50', 'pwgtp51', 'pwgtp52', 'pwgtp53', 'pwgtp54', 'pwgtp55', 'pwgtp56', 'pwgtp57', 'pwgtp58', 'pwgtp59', 'pwgtp60', 'pwgtp61', 'pwgtp62', 'pwgtp63', 'pwgtp64', 'pwgtp65', 'pwgtp66', 'pwgtp67', 'pwgtp68', 'pwgtp69', 'pwgtp70', 'pwgtp71', 'pwgtp72', 'pwgtp73', 'pwgtp74', 'pwgtp75', 'pwgtp76', 'pwgtp77', 'pwgtp78', 'pwgtp79', 'pwgtp80']\n",
      "\n",
      "No wonder they provide a reference dictionary to figure out what all these acronyms mean\n"
     ]
    }
   ],
   "source": [
    "print('The 2009 American Community Survey has a lot of data, look at all these columns! \\n\\n'+str(list(merged.columns.values)))\n",
    "print('\\nNo wonder they provide a reference dictionary to figure out what all these acronyms mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously there is way more data here than we need so lets start by reducing the number of columns to only what we consider useful for our classification. The following attributes will remain as they are peices of information that new hires might give away when applying for a job.\n",
    "\n",
    "Citizenship, age, class of work, English fluentcy, martial status, military status, sex, education background, disability status, race, geographical location. We will also keep the individual's income as it is the attribute that we are attempting to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3030728 entries, 0 to 1466654\n",
      "Data columns (total 13 columns):\n",
      "CIT      int64\n",
      "AGEP     int64\n",
      "COW      float64\n",
      "ENG      float64\n",
      "MAR      int64\n",
      "MIL      float64\n",
      "SCHL     float64\n",
      "SEX      int64\n",
      "DIS      int64\n",
      "PINCP    float64\n",
      "POWSP    float64\n",
      "RAC1P    int64\n",
      "FOD1P    float64\n",
      "dtypes: float64(7), int64(6)\n",
      "memory usage: 323.7 MB\n"
     ]
    }
   ],
   "source": [
    "cols_to_save = ['CIT','AGEP','COW','ENG','MAR','MIL','SCHL','SEX','DIS','PINCP','POWSP','RAC1P','FOD1P']\n",
    "new_data = merged.filter(items=cols_to_save)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIT</th>\n",
       "      <th>AGEP</th>\n",
       "      <th>COW</th>\n",
       "      <th>ENG</th>\n",
       "      <th>MAR</th>\n",
       "      <th>MIL</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>DIS</th>\n",
       "      <th>PINCP</th>\n",
       "      <th>POWSP</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>FOD1P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>36800.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>56000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>41000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>14200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20500.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>66000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>45000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>97520.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3606.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466625</th>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466626</th>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466627</th>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466628</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466629</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466630</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466631</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466632</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466633</th>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466634</th>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466635</th>\n",
       "      <td>1</td>\n",
       "      <td>81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>28700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466636</th>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466637</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>53400.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466638</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466639</th>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>41100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466640</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>46000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466641</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>22000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466642</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13020.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466643</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466644</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>36500.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466645</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466646</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466647</th>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466648</th>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>140000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466649</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466650</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466651</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-4800.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466652</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>39000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466653</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>23000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466654</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3030728 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CIT  AGEP  COW  ENG  MAR  MIL  SCHL  SEX  DIS     PINCP  POWSP  \\\n",
       "0          1    51  NaN  NaN    4  3.0  16.0    1    1    3800.0    NaN   \n",
       "1          1    64  1.0  NaN    1  5.0  19.0    2    1   36800.0    1.0   \n",
       "2          1    68  1.0  NaN    1  3.0  14.0    1    1   54600.0    1.0   \n",
       "3          1    61  1.0  NaN    3  5.0  16.0    2    2    6000.0    1.0   \n",
       "4          1    38  1.0  NaN    5  5.0  16.0    1    2   14000.0    1.0   \n",
       "5          1    65  6.0  NaN    1  5.0  16.0    2    1   13000.0    NaN   \n",
       "6          1    74  1.0  NaN    1  3.0  16.0    1    1   45200.0    1.0   \n",
       "7          1    23  1.0  NaN    5  5.0  12.0    2    1     820.0    NaN   \n",
       "8          1    42  1.0  NaN    1  5.0  19.0    2    2   25200.0    1.0   \n",
       "9          1    42  1.0  NaN    1  5.0  13.0    1    2   56000.0    1.0   \n",
       "10         1    23  1.0  NaN    3  5.0  18.0    2    2       0.0    NaN   \n",
       "11         1     3  NaN  NaN    5  NaN   2.0    2    2       NaN    NaN   \n",
       "12         1    35  3.0  NaN    3  5.0  19.0    1    2   45000.0    1.0   \n",
       "13         1    29  1.0  NaN    3  5.0  19.0    2    2     300.0    NaN   \n",
       "14         1    39  1.0  NaN    1  5.0  16.0    1    2   41000.0    1.0   \n",
       "15         1    38  1.0  NaN    1  5.0  16.0    2    2    3500.0    1.0   \n",
       "16         1    14  NaN  NaN    5  NaN  11.0    1    2       NaN    NaN   \n",
       "17         1    13  NaN  NaN    5  NaN   9.0    2    2       NaN    NaN   \n",
       "18         1    12  NaN  NaN    5  NaN   8.0    2    2       NaN    NaN   \n",
       "19         1    32  NaN  NaN    5  5.0  12.0    1    2    1500.0    NaN   \n",
       "20         1    34  4.0  NaN    5  5.0  16.0    2    2   14200.0    1.0   \n",
       "21         1    14  NaN  NaN    5  NaN  10.0    2    2       NaN    NaN   \n",
       "22         1    12  NaN  NaN    5  NaN   8.0    1    2       NaN    NaN   \n",
       "23         1    72  NaN  NaN    1  3.0  17.0    1    1   12000.0    NaN   \n",
       "24         1    60  1.0  NaN    1  5.0  16.0    2    2       0.0    NaN   \n",
       "25         1    39  7.0  NaN    3  5.0  19.0    1    1   20500.0    1.0   \n",
       "26         1    53  1.0  NaN    1  5.0  21.0    2    2   66000.0    1.0   \n",
       "27         1    54  1.0  NaN    1  5.0  19.0    1    2   45000.0    1.0   \n",
       "28         1    21  1.0  NaN    5  5.0  16.0    1    2    6000.0    NaN   \n",
       "29         1    51  2.0  NaN    5  5.0  24.0    1    2   97520.0    1.0   \n",
       "...      ...   ...  ...  ...  ...  ...   ...  ...  ...       ...    ...   \n",
       "1466625    1    75  NaN  NaN    2  5.0  16.0    2    2   37500.0    NaN   \n",
       "1466626    5    30  1.0  2.0    1  5.0   9.0    1    2   50000.0   56.0   \n",
       "1466627    5    26  NaN  2.0    1  5.0  15.0    2    2       0.0    NaN   \n",
       "1466628    1     4  NaN  NaN    5  NaN   1.0    1    2       NaN    NaN   \n",
       "1466629    1     2  NaN  NaN    5  NaN   NaN    2    2       NaN    NaN   \n",
       "1466630    1    48  1.0  NaN    1  5.0  21.0    1    2   80000.0   56.0   \n",
       "1466631    1    53  1.0  NaN    1  5.0  19.0    2    2    3000.0   56.0   \n",
       "1466632    1    25  1.0  NaN    5  5.0  18.0    1    2    3700.0   30.0   \n",
       "1466633    1    93  NaN  NaN    2  5.0  16.0    2    1   12000.0    NaN   \n",
       "1466634    1    64  4.0  NaN    2  5.0  16.0    2    1   10800.0    NaN   \n",
       "1466635    1    81  NaN  NaN    1  3.0  19.0    1    2   28700.0    NaN   \n",
       "1466636    1    78  NaN  NaN    1  5.0  16.0    2    2    6800.0    NaN   \n",
       "1466637    1    79  NaN  NaN    2  3.0  21.0    1    2   53400.0    NaN   \n",
       "1466638    1    87  NaN  NaN    2  5.0  12.0    2    1    4200.0    NaN   \n",
       "1466639    1    79  NaN  NaN    2  5.0  18.0    2    2   41100.0    NaN   \n",
       "1466640    1    49  2.0  NaN    1  5.0  18.0    2    2   46000.0   56.0   \n",
       "1466641    1    55  1.0  NaN    1  5.0  16.0    1    2   22000.0   56.0   \n",
       "1466642    1    24  3.0  NaN    1  5.0  21.0    2    2   13020.0   56.0   \n",
       "1466643    1     1  NaN  NaN    5  NaN   NaN    2    2       NaN    NaN   \n",
       "1466644    1    29  3.0  NaN    1  5.0  21.0    1    2   36500.0   56.0   \n",
       "1466645    1    56  NaN  NaN    3  5.0  18.0    2    2    6000.0    NaN   \n",
       "1466646    1    42  1.0  NaN    3  5.0  21.0    2    2    4000.0   56.0   \n",
       "1466647    1    41  1.0  NaN    1  5.0  18.0    2    2       0.0    NaN   \n",
       "1466648    1    42  1.0  NaN    1  3.0  19.0    1    2  140000.0   56.0   \n",
       "1466649    1     2  NaN  NaN    5  NaN   NaN    1    2       NaN    NaN   \n",
       "1466650    1    50  NaN  NaN    3  4.0  21.0    1    1   15000.0    NaN   \n",
       "1466651    1    60  6.0  NaN    1  3.0  19.0    1    2   -4800.0   56.0   \n",
       "1466652    1    58  6.0  NaN    1  5.0  19.0    2    2   39000.0   56.0   \n",
       "1466653    1    24  3.0  NaN    1  5.0  19.0    2    2   23000.0   56.0   \n",
       "1466654    1    23  1.0  1.0    1  5.0  19.0    1    2   10000.0   56.0   \n",
       "\n",
       "         RAC1P   FOD1P  \n",
       "0            1     NaN  \n",
       "1            1     NaN  \n",
       "2            1     NaN  \n",
       "3            2     NaN  \n",
       "4            2     NaN  \n",
       "5            1     NaN  \n",
       "6            1     NaN  \n",
       "7            1     NaN  \n",
       "8            1     NaN  \n",
       "9            1     NaN  \n",
       "10           1     NaN  \n",
       "11           1     NaN  \n",
       "12           1     NaN  \n",
       "13           1     NaN  \n",
       "14           1     NaN  \n",
       "15           1     NaN  \n",
       "16           1     NaN  \n",
       "17           1     NaN  \n",
       "18           1     NaN  \n",
       "19           2     NaN  \n",
       "20           2     NaN  \n",
       "21           2     NaN  \n",
       "22           2     NaN  \n",
       "23           1     NaN  \n",
       "24           1     NaN  \n",
       "25           1     NaN  \n",
       "26           2  6209.0  \n",
       "27           2     NaN  \n",
       "28           2     NaN  \n",
       "29           1  3606.0  \n",
       "...        ...     ...  \n",
       "1466625      1     NaN  \n",
       "1466626      1     NaN  \n",
       "1466627      1     NaN  \n",
       "1466628      1     NaN  \n",
       "1466629      1     NaN  \n",
       "1466630      1  3600.0  \n",
       "1466631      1     NaN  \n",
       "1466632      1     NaN  \n",
       "1466633      1     NaN  \n",
       "1466634      1     NaN  \n",
       "1466635      1     NaN  \n",
       "1466636      1     NaN  \n",
       "1466637      1  5501.0  \n",
       "1466638      1     NaN  \n",
       "1466639      1     NaN  \n",
       "1466640      1     NaN  \n",
       "1466641      1     NaN  \n",
       "1466642      1  2304.0  \n",
       "1466643      1     NaN  \n",
       "1466644      1  1103.0  \n",
       "1466645      1     NaN  \n",
       "1466646      1  6107.0  \n",
       "1466647      9     NaN  \n",
       "1466648      1     NaN  \n",
       "1466649      1     NaN  \n",
       "1466650      1  1105.0  \n",
       "1466651      1     NaN  \n",
       "1466652      1     NaN  \n",
       "1466653      1     NaN  \n",
       "1466654      1     NaN  \n",
       "\n",
       "[3030728 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We began with over 3 million records and 279 differnt attributes provided in the survey. Of the nearly 300 attributes that were provided in the dataset, we reduced it down to the thirteen shown above. The table below shows the desired attributes and examples of what we would want the data to look like. \n",
    "\n",
    "|Attribute|Description|Type|Example|\n",
    "|:---:|:---:|:---:|:---:|\n",
    "| CIT | Citizenship Status | Int | 1. Citizen, 0. Non-citizen |\n",
    "| AGEP | Age | Int | 23\n",
    "| COW | Class of Worker | Float | 3. Local Government, 4. State Government |\n",
    "| ENG | Ability to speak English  | Int | 1. Speaks English, 0. Doesn't Speak English |\n",
    "| MAR | Marital Status | Int | 1. Married, 2. Widowed |\n",
    "| MIL | Military Service | Int | 1. Yes, 0. No |\n",
    "| SCHL | Educational Attainment  | Float | 21 Bachelor's Degree, 22 Master's Degree |\n",
    "| SEX | Sex      | Int | True. Male |\n",
    "| DIS | Disability | Int | True. Disabled |\n",
    "| PINCP | Total Person's Income | Float\n",
    "| POWSP | Place of work | Float | 048 Texas, 049 Utah |\n",
    "| RAC1P | Detailed Race Code | Int | 1 White, 6 Asian |\n",
    "| FOD1P | Field of Degree | Float | 2407 Computer Engineering, 2408 Electrical Engineering |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "The next step is data cleaning and modifying the data into more useful data types. Lets start by modifying some of these data types into more useful and proper variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change citizenship to Int.\n",
    "# 1-4 is a citizen (true) and 5 is not a citizen (false)\n",
    "\n",
    "new_data.CIT.replace(to_replace = range(5),\n",
    "                    value=[1,1,1,1,0],\n",
    "                    inplace=True)\n",
    "new_data['CIT'] = new_data['CIT'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Ability to Speak English to boolean\n",
    "# b is N/A but it would be a good assumption to assume they speak English\n",
    "new_data['ENG']=new_data['ENG'].fillna(1)\n",
    "# 1-2 speaks English well or very well, 3-4 speaks English not well or not at all.\n",
    "new_data.ENG.replace(to_replace = range(4),\n",
    "                    value=[1,1,0,0],\n",
    "                    inplace=True)\n",
    "new_data['ENG'] = new_data['ENG'].astype('int')# Change Military Status to Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b is N/A because less than 17 years old so lets just change this to 0\n",
    "new_data['MIL']=new_data['MIL'].fillna(0)\n",
    "# 1-3 Yes, 4-5 No\n",
    "new_data.MIL.replace(to_replace = range(5),\n",
    "                    value=[1,1,1,0,0],\n",
    "                    inplace=True)\n",
    "new_data['MIL'] = new_data['MIL'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change Sex to Int\n",
    "# # 1 is male, 2 is female. Changing 2 to 0 for boolean conversion\n",
    "# new_data.SEX.replace(to_replace = range(2),\n",
    "#                     value=[1,0],\n",
    "#                     inplace=True)\n",
    "# new_data['SEX'] = new_data['SEX'].astype('Int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Change DIS to Int\n",
    "# # 1 is disabled, 2 is no disability. Changing 2 to 0 for boolean conversion\n",
    "# new_data.DIS.replace(to_replace = range(2),\n",
    "#                     value=[1,0],\n",
    "#                     inplace=True)\n",
    "# new_data['DIS'] = new_data['DIS'].astype('Int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Educational Atttainment to INT\n",
    "# bb is N/A for less than 3 years old.\n",
    "new_data['SCHL']=new_data['SCHL'].fillna(0)\n",
    "# For this classification lets simplify some of these education levels.\n",
    "# 0 between No schooling and Grade 8\n",
    "# 1 between Grade 9 and Grade 12 no diploma\n",
    "# 2 for High School degree or GED\n",
    "# 3 Some college to Associate's degree\n",
    "# 4 Bachelor's Degree\n",
    "# 5 Master's Degree\n",
    "# 6 Professional degree or Doctorate\n",
    "new_data.SCHL.replace(to_replace = range(25),\n",
    "                    value=[0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,2,2,3,3,3,4,5,6,6],\n",
    "                    inplace=True)\n",
    "new_data['SCHL'] = new_data['SCHL'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets remove all entries with people under the age of 18 because our goal is to focus on personal income of working class individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete younger than 18\n",
    "new_data = new_data[new_data.AGEP >= 18]\n",
    "#new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets work on the Nulls..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that contain nulls: ['COW', 'POWSP', 'FOD1P']\n"
     ]
    }
   ],
   "source": [
    "# find null columns\n",
    "print('Columns that contain nulls: '+str(new_data.columns[new_data.isnull().any()].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Field of Study, we can replace all the Nulls with 0s since they only refer to those with at least a college degree\n",
    "Let's remove any Class of Worker and Place of Work rows with Nulls since those entres are for idividuals who have not worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4355: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "# Field of Study  -> 0\n",
    "# Class of Worker -> Remove if Null\n",
    "# Place of Work   -> Remove if Null\n",
    "\n",
    "new_data['FOD1P'].fillna(0, inplace=True)\n",
    "new_data = new_data[pd.notnull(new_data['COW'])]\n",
    "new_data = new_data[pd.notnull(new_data['POWSP'])]\n",
    "\n",
    "# Convert the Floats to Ints\n",
    "# COW, POWSP, FOD1P, PINCP\n",
    "\n",
    "new_data['COW'] = new_data['COW'].astype('int')\n",
    "new_data['POWSP'] = new_data['POWSP'].astype('int')\n",
    "new_data['FOD1P'] = new_data['FOD1P'].astype('int')\n",
    "new_data['PINCP'] = new_data['PINCP'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that contain nulls: []\n"
     ]
    }
   ],
   "source": [
    "# Double check for null columns\n",
    "print('Columns that contain nulls: '+str(new_data.columns[new_data.isnull().any()].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by splitting the personal income data into 2 different classes to test Binary Logistic Regression. Later, we will split the personal income into 5 different classes. This will help verify our income predictions while also being specific enough to work with our business goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0]\n",
       "Categories (2, int64): [0 < 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_data = new_data.copy(deep=False) # saving a copy for later\n",
    "\n",
    "new_data['PINCP'] = pd.qcut(new_data.PINCP, 2, labels=[0,1])\n",
    "new_data['PINCP'].unique()\n",
    "\n",
    "#qcut(x, q, labels=None, retbins=False, precision=3, duplicates='raise')\n",
    "#cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False)\n",
    "\n",
    "#new_data['PINCP'] = pd.qcut(new_data.PINCP, 5, labels=[0,1,2,3,4])\n",
    "#new_data['PINCP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data['PINCP'] = new_data['PINCP'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in each class:\n",
      "1: 660205\n",
      "0: 678074\n"
     ]
    }
   ],
   "source": [
    "# Lets see how the income classes have split...\n",
    "print('Number of people in each class:')\n",
    "for value in new_data.PINCP.unique(): \n",
    "    print(str(value)+': ' +str(len(new_data[new_data['PINCP'] == value])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1338279 entries, 1 to 1466654\n",
      "Data columns (total 13 columns):\n",
      "Citizenship        1338279 non-null int32\n",
      "Age                1338279 non-null int64\n",
      "Class of Work      1338279 non-null int32\n",
      "Speaks English     1338279 non-null int32\n",
      "Martial Status     1338279 non-null int64\n",
      "Military Status    1338279 non-null int32\n",
      "Education Level    1338279 non-null int32\n",
      "Male               1338279 non-null int64\n",
      "Disabled?          1338279 non-null int64\n",
      "Income             1338279 non-null int32\n",
      "Place of Work      1338279 non-null int32\n",
      "Race               1338279 non-null int64\n",
      "Field of Study     1338279 non-null int32\n",
      "dtypes: int32(8), int64(5)\n",
      "memory usage: 102.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's rename some of these columns so they make more sense.\n",
    "new_data.rename(columns={'CIT': 'Citizenship','AGEP': 'Age','COW': 'Class of Work','ENG': 'Speaks English','MAR': 'Martial Status','MIL': 'Military Status','SCHL': 'Education Level','SEX': 'Male','DIS': 'Disabled?','PINCP': 'Income','POWSP': 'Place of Work','RAC1P': 'Race','FOD1P': 'Field of Study'}, inplace=True)\n",
    "new_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there is still a significant amount of data for testing and training purposes (Over 1.3 million entries). The next step is to determine how much of the data should be used immediately for training purposes and how much should be saved until the end for testing purposes. \n",
    "\n",
    "An 80/20 split seems to be a good number for this situation. As long as the split is randomized and there would be over a million entries just for training, and around a quarter of a million entries for testing. That seems like plenty of data to create a accurate model as well as test the model for it's accuracy. Furthermore, since we will be spliting the income into five classes, the 80/20 split seems appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=1, random_state=None, test_size=0.2, train_size=0.8)\n"
     ]
    }
   ],
   "source": [
    "# Lets start by making a copy of the cleaned data we are using.\n",
    "new_df = new_data.copy()\n",
    "if 'Income' in new_data:\n",
    "    y = new_df['Income'].values    # Since Income is our target class, lets make a copy of it\n",
    "    del new_df['Income']           # Now we need to remove the target class\n",
    "    X = new_df.values              # The remaining data will be used to train\n",
    "\n",
    "# Scikit Learns provides a way to split our data into training and testing subsets.\n",
    "cv_object = ShuffleSplit(train_size=.8, test_size=0.2, n_splits=1)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to create a custom, one-versus-all logistic regression classifier using numpy and scipy to optimize. \n",
    "Let's start by using the template from: https://github.com/eclarson/MachineLearningNotebooks/blob/master/05.%20Logistic%20Regression.ipynb\n",
    "\n",
    "We will add and modify functions as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "class BinaryLogisticRegressionBase:\n",
    "    # private:\n",
    "    def __init__(self, eta, iterations=20, c=0.001, norm=0.5):\n",
    "        self.eta = eta\n",
    "        self.iters = iterations\n",
    "        self.c = c\n",
    "        if((0 <= norm <= 1) or ( norm == -1)): #Check to see if the l1 norm is valid\n",
    "            self.norm = norm\n",
    "        else: raise ValueError(\"Norm must be between 0 and 1 or 0 and -1\")\n",
    "        # internally we will store the weights as self.w_ to keep with sklearn conventions\n",
    "        \n",
    "    def __str__(self):\n",
    "        if(hasattr(self,'w_')):\n",
    "            return 'Binary Logistic Regression Object with coefficients:\\n'+ str(self.w_) # is we have trained the object\n",
    "        else:\n",
    "            return 'Untrained Binary Logistic Regression Object'\n",
    "    \n",
    "    # convenience, private:\n",
    "    @staticmethod\n",
    "    def _sigmoid(theta):\n",
    "        return 1/(1+np.exp(-theta)) \n",
    "    \n",
    "    @staticmethod\n",
    "    def _add_bias(x):\n",
    "        return np.hstack((np.ones((x.shape[0],1)),x)) # add bias term\n",
    "     \n",
    "    # public:\n",
    "    def _normalize(self, gradient):\n",
    "        #Implementation for Elastic Net regularization \n",
    "        sub_1 = np.copy(gradient[1:])\n",
    "        sub_2 = np.copy(gradient[1:])\n",
    "        \n",
    "        #Calculate L1 Norm \n",
    "        mask = np.logical_and(sub_1 >= (-self.c/2),sub_1 <= (self.c/2))\n",
    "        sub_1[mask] = 0\n",
    "        sub_1[sub_1 < (-self.c/2)] += (self.c / 2)\n",
    "        sub_1[sub_1 > (self.c/2)] -= (self.c / 2)\n",
    "        \n",
    "        #Calculate L2 Norm\n",
    "        sub_2 += -2 * self.w_[1:] * self.c\n",
    "        \n",
    "        #Combine the regularizations to make an elastic net.\n",
    "        gradient[1:] = self.norm * sub_1 + (1-self.norm) * sub_2\n",
    "        return gradient\n",
    "    \n",
    "    def newtonNormalize(self, w, gradient):\n",
    "        # regularization (adds both if 3)\n",
    "        if self.norm & 1: # L1 norm\n",
    "            gradient[1:] += -1 * w[1:] * self.c\n",
    "        elif self.norm & 2: # L2 norm\n",
    "            gradient[1:] += -2 * w[1:] * self.c\n",
    "            \n",
    "    def fit(self, x, y):\n",
    "        Xb = x\n",
    "        #Xb = self._add_bias(x) # add bias term\n",
    "        num_samples, num_features = Xb.shape\n",
    "        \n",
    "        self.w_ = np.zeros((x.shape[1],1)) # init weight vector to zeros\n",
    "        \n",
    "        # for as many as the max iterations\n",
    "        for _ in range(self.iters):\n",
    "            gradient = self._get_gradient(Xb,y)\n",
    "            #self.normalize(self.w_, gradient)\n",
    "            self.w_ += gradient*self.eta # multiply by learning rate \n",
    "    \n",
    "    def predict_proba(self,X,add_bias=True):\n",
    "        # add bias term if requested\n",
    "        #Xb = self._add_bias(X) if add_bias else X\n",
    "        Xb = X\n",
    "        return self._sigmoid(Xb @ self.w_) # return the probability y=1\n",
    "    \n",
    "    def predict(self,x):\n",
    "        return (self.predict_proba(x)>0.5) #return the actual prediction\n",
    "    \n",
    "    # vectorized gradient calculation with regularization using L2 Norm\n",
    "    def _get_gradient(self,X,y):\n",
    "        ydiff = y-self.predict_proba(X,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(X * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if (self.norm == -1):\n",
    "            return gradient\n",
    "        return self._normalize(gradient)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write some classifiers (Steepest Descent, Stochastic Gradient Descent, and Newton's Method)\n",
    "\n",
    "# Steepest Gradient Descent algorithm\n",
    "class BinarySteepDescClassifier(BinaryLogisticRegressionBase):\n",
    "    def _get_gradient(self, x, y):\n",
    "        ydiff = y-self.predict_proba(x,add_bias=False).ravel()\n",
    "        gradient = np.mean(x * ydiff[:,np.newaxis], axis=0)\n",
    "        return gradient.reshape(self.w_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent Algorithm\n",
    "class BinaryStochDescClassifier(BinaryLogisticRegressionBase):\n",
    "    def _get_gradient(self, x, y):\n",
    "        ydiff = y-self.predict_proba(x,add_bias=False).ravel() # get y difference\n",
    "        gradient = np.mean(x * ydiff[:,np.newaxis], axis=0) # make ydiff a column vector and multiply through\n",
    "        gradient = gradient.reshape(self.w_.shape)\n",
    "        if (self.norm == -1):\n",
    "            return gradient\n",
    "        return self._normalize(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's Method (BFGS)\n",
    "class BinaryNewtonClassifier(BinaryLogisticRegressionBase):\n",
    "#     def _hessian(self,x,y):\n",
    "#         g = self.predict_proba(x,add_bias=False).ravel()\n",
    "#         if (self.norm == -1):\n",
    "#             return x.T @ np.diag(g*(1-g)) @ x - 2\n",
    "#         else:\n",
    "#             return x.T @ np.diag(g*(1-g)) @ x - 2 * (1-self.norm) * self.c\n",
    "    \n",
    "#     def fit(self, x, y):\n",
    "#         Xb = self._add_bias(x)\n",
    "#         num_samples, num_features = Xb.shape\n",
    "#         self.w_ = np.zeros((num_features,1))\n",
    "#         for _ in range(self.iters):\n",
    "#             gradient = self._get_gradient(Xb,y)\n",
    "#             inv_hessian = np.linalg.inv(self._hessian(Xb,y))\n",
    "#             self.w_ +=  inv_hessian@gradient*self.eta \n",
    "    def fit(self, x, y):\n",
    "        def obj_fn(w, x, y, c):\n",
    "            g = expit(x @ w)\n",
    "            return -np.sum(np.log(g[y==1]))-np.sum(np.log(1-g[y==0])) + c*sum(w**2)\n",
    "        \n",
    "        def obj_grad(w, x, y, c):\n",
    "            g = expit(x @ w)\n",
    "            ydiff = y-g\n",
    "            gradient = np.mean(x * ydiff[:,np.newaxis], axis=0)\n",
    "            gradient = gradient.reshape(w.shape)\n",
    "            self.newtonNormalize(w, gradient)\n",
    "            return -gradient\n",
    "        \n",
    "        self.w_ = fmin_bfgs(obj_fn, \n",
    "                            np.zeros((x.shape[1], 1)), \n",
    "                            fprime=obj_grad, \n",
    "                            args=(x, y, self.c), \n",
    "                            gtol=1e-03, \n",
    "                            maxiter=self.iters,\n",
    "                            disp=False).reshape((x.shape[1], 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by using scikit-learn's logistic refression classifier as a base of reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Iteration 0 ====\n",
      "Wall time: 9.5 s\n",
      "Accuracy for income bracket 0 : 0.7329644129161393\n",
      "Accuracy for income bracket 1 : 0.7028095686226891\n",
      "Overall accuracy: 0.7181195265564755\n",
      "Confusion matrix\n",
      " [[99604 36288]\n",
      " [39159 92605]]\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "\n",
    "# We will use the cv_object we created earilier and iterate through the different training and testing sets.\n",
    "\n",
    "iter_num=0\n",
    "for train_indices, test_indices in cv_object.split(X):\n",
    "    print(\"==== Iteration\",iter_num,\"====\")\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    %time lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    for i in range(0,len(conf)):\n",
    "        print(\"Accuracy for income bracket\", i, \":\",conf[i][i]/sum(conf[i]))\n",
    "    print(\"Overall accuracy:\", acc )\n",
    "    print(\"Confusion matrix\\n\",conf)\n",
    "    iter_num += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our base accuracy is around 72%. Note that this accuracy will vary everytime you run the above code.\n",
    "Let's now test our Binary classifiers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-3.13174691]\n",
      " [ 1.72797173]\n",
      " [-1.06074975]\n",
      " [-0.27770514]\n",
      " [-6.21497795]\n",
      " [-5.01860433]\n",
      " [ 4.19562217]\n",
      " [-5.17155836]\n",
      " [-0.47226692]\n",
      " [ 0.15690308]\n",
      " [-1.96547819]\n",
      " [98.72188756]]\n",
      "Accuracy:  0.6044337540976135\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Let's start with Steepest Descent Classifer\n",
    "steep = BinarySteepDescClassifier(0.1,1000, c=0.001,norm=1)\n",
    "\n",
    "steep.fit(X,y)\n",
    "yhat = steep.predict(X)\n",
    "print (steep)\n",
    "print('Accuracy: ' , accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[-3.16080651]\n",
      " [ 1.74469006]\n",
      " [-1.04963685]\n",
      " [-0.29419391]\n",
      " [-6.37202969]\n",
      " [-5.07017894]\n",
      " [ 4.18532568]\n",
      " [-5.18448481]\n",
      " [-0.48902674]\n",
      " [ 0.26390455]\n",
      " [-1.99068996]\n",
      " [98.74388974]]\n",
      "Accuracy:  0.6000968407932875\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now Let's do Stochastic Descent Classifer\n",
    "stoch = BinaryStochDescClassifier(0.1,1000, c=0.001,norm=1)\n",
    "\n",
    "stoch.fit(X,y)\n",
    "yhat = stoch.predict(X)\n",
    "print (stoch)\n",
    "print('Accuracy: ' , accuracy_score(y,yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Nick\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Logistic Regression Object with coefficients:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Accuracy:  0.5066761116329256\n",
      "Wall time: 2.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Now Let's do Newton's Method Classifer (BFGS)\n",
    "newton = BinaryNewtonClassifier(0.1,10, c=0.001,norm=0)\n",
    "\n",
    "newton.fit(X,y)\n",
    "yhat = newton.predict(X)\n",
    "print (newton)\n",
    "print('Accuracy: ' , accuracy_score(y,yhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that the above cells were run using binary logistic regression. In this dataset it's goal is to determine whether a person is in the top income bracket or the bottom income bracket. Although it is not entirely useful, starting from these binary classifers we can build a custom multi class logistic regression classifier for our intended goal of 5 different income brackets. \n",
    "\n",
    "We will also need to optimize these regression classifiers as we can see above they are less efficient than the logistic regression classifier from sklearn. First thing to do is to bring in our old copy of the data so we can divide the income brackets into five classes instead of two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bin edges must be unique: array([0., 0., 1., 2., 3., 4.]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-93c5c3ab8f5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfuture_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PINCP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqcut\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuture_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPINCP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfuture_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PINCP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfuture_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PINCP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfuture_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PINCP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\tile.py\u001b[0m in \u001b[0;36mqcut\u001b[1;34m(x, q, labels, retbins, precision, duplicates)\u001b[0m\n\u001b[0;32m    206\u001b[0m     fac, bins = _bins_to_cuts(x, bins, labels=labels,\n\u001b[0;32m    207\u001b[0m                               \u001b[0mprecision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minclude_lowest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m                               dtype=dtype, duplicates=duplicates)\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     return _postprocess_for_cut(fac, bins, retbins, x_is_series,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\tile.py\u001b[0m in \u001b[0;36m_bins_to_cuts\u001b[1;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates)\u001b[0m\n\u001b[0;32m    232\u001b[0m             raise ValueError(\"Bin edges must be unique: {bins!r}.\\nYou \"\n\u001b[0;32m    233\u001b[0m                              \u001b[1;34m\"can drop duplicate edges by setting \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m                              \"the 'duplicates' kwarg\".format(bins=bins))\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m             \u001b[0mbins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_bins\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Bin edges must be unique: array([0., 0., 1., 2., 3., 4.]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg"
     ]
    }
   ],
   "source": [
    "future_data['PINCP'] = pd.qcut(future_data.PINCP, 5, labels=[0,1,2,3,4])\n",
    "future_data['PINCP'].unique()\n",
    "\n",
    "future_data['PINCP'] = future_data['PINCP'].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of people in each class:\n",
      "2: 233964\n",
      "3: 268048\n",
      "0: 267705\n",
      "1: 302608\n",
      "4: 265954\n"
     ]
    }
   ],
   "source": [
    "# Lets see how the income classes have split...\n",
    "print('Number of people in each class:')\n",
    "for value in future_data.PINCP.unique(): \n",
    "    print(str(value)+': ' +str(len(future_data[future_data['PINCP'] == value])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1338279 entries, 1 to 1466654\n",
      "Data columns (total 13 columns):\n",
      "Citizenship        1338279 non-null int32\n",
      "Age                1338279 non-null int64\n",
      "Class of Work      1338279 non-null int32\n",
      "Speaks English     1338279 non-null int32\n",
      "Martial Status     1338279 non-null int64\n",
      "Military Status    1338279 non-null int32\n",
      "Education Level    1338279 non-null int32\n",
      "Male               1338279 non-null int64\n",
      "Disabled?          1338279 non-null int64\n",
      "Income             1338279 non-null int32\n",
      "Place of Work      1338279 non-null int32\n",
      "Race               1338279 non-null int64\n",
      "Field of Study     1338279 non-null int32\n",
      "dtypes: int32(8), int64(5)\n",
      "memory usage: 102.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's rename some of these columns so they make more sense.\n",
    "future_data.rename(columns={'CIT': 'Citizenship','AGEP': 'Age','COW': 'Class of Work','ENG': 'Speaks English','MAR': 'Martial Status','MIL': 'Military Status','SCHL': 'Education Level','SEX': 'Male','DIS': 'Disabled?','PINCP': 'Income','POWSP': 'Place of Work','RAC1P': 'Race','FOD1P': 'Field of Study'}, inplace=True)\n",
    "future_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=1, random_state=None, test_size=0.2, train_size=0.8)\n"
     ]
    }
   ],
   "source": [
    "# Lets start by making a copy of the cleaned data we are using.\n",
    "future_df = future_data.copy()\n",
    "if 'Income' in future_data:\n",
    "    y = future_df['Income'].values    # Since Income is our target class, lets make a copy of it\n",
    "    del future_df['Income']           # Now we need to remove the target class\n",
    "    X = future_df.values              # The remaining data will be used to train\n",
    "\n",
    "# Scikit Learns provides a way to split our data into training and testing subsets.\n",
    "cv_object = ShuffleSplit(train_size=.8, test_size=0.2, n_splits=1)\n",
    "                         \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Class Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
